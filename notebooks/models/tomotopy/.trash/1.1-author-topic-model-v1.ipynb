{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5addb7c1-4740-43b9-a51c-7f96f84429d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efd64f0-f21c-482c-b4a7-37a7b9f72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcf7222-d029-47b3-80fb-35740c7da4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.abspath('../../..') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f505c7-6895-4797-b928-9d346d8057f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    preprocess_string, \n",
    "    strip_tags,\n",
    "    strip_punctuation, \n",
    "    strip_multiple_whitespaces, \n",
    "    strip_numeric, \n",
    "    lower_to_unicode,\n",
    "    strip_short,\n",
    "    remove_stopwords,\n",
    ")\n",
    "import tomotopy as tp\n",
    "\n",
    "from src.dataset import load_tweets, load_availability\n",
    "from src.utils.text import Document, Dictionary\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c091be4-082f-4776-bb87-101ee3ce8e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avx2\n"
     ]
    }
   ],
   "source": [
    "print(tp.isa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dafe1d78-649f-41ac-a443-b0275cea3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(path=None):\n",
    "    if path is None:\n",
    "        path = '../../../data/external/keywords.v3.2.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    output = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        phrase, topic = row.raw_keyword, row.label\n",
    "        for token in phrase.split(' '):\n",
    "            if len(token) > 2:\n",
    "                output[topic].add(token)\n",
    "    return {k: list(v) for k, v in output.items()}\n",
    "\n",
    "keywords = load_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6727ff23-b431-40a2-b29b-3e7af582de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Documents: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 820202/820202 [00:45<00:00, 18160.77it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_documents(path=None):\n",
    "    if path is None:\n",
    "        path = '../../../data/interim/models/tweets_intra_subject_analysis.jsonl'\n",
    "    with open(path, 'r') as fp:\n",
    "        for line in tqdm.tqdm(fp.readlines(), desc='Loading Documents'):\n",
    "            record = json.loads(line)\n",
    "            text = record['tweet']['text']\n",
    "            author = record['subject_id']\n",
    "            # author = '{}-{}-{}'.format(record['subject_id'], record['event_id'], record['period'])\n",
    "            d = Document(text=text, author=author)\n",
    "            yield d\n",
    "\n",
    "url_pattern = re.compile('http[s]?://\\S+')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for doc in load_documents():\n",
    "    tokens = preprocess_string(doc.text, filters=[\n",
    "        lower_to_unicode,\n",
    "        lambda x: url_pattern.sub(' ', x),\n",
    "        strip_tags,\n",
    "        strip_punctuation,\n",
    "        strip_numeric, \n",
    "        remove_stopwords,\n",
    "        strip_short,\n",
    "        strip_multiple_whitespaces, \n",
    "    ])\n",
    "    doc.set_tokens(tokens)\n",
    "    if len(doc.tokens) > 0:\n",
    "        corpus.append(doc)\n",
    "        \n",
    "vocab = Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e9b46a-e187-4bda-8e99-6714c519db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../../../data/interim/models/author2doc.json'):\n",
    "    with open('../../../data/interim/models/author2doc.json', 'r', encoding='utf-8') as fp:\n",
    "        author2doc = json.load(fp)\n",
    "else:\n",
    "    author2doc = defaultdict(list)\n",
    "    for i, doc in enumerate(tqdm.tqdm(results, desc='Extracting Tokens')):\n",
    "        author2doc[doc.author].append(i)\n",
    "    author2doc = dict(author2doc)\n",
    "    with open('../../../data/interim/models/author2doc.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(author2doc, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cec81e9-c916-410b-9d57-23b82821592b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 318470)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_topic_word_mat(keywords, vocab, num_topics, pseudo_count=1e7):\n",
    "    # create a (ntopics, nterms) matrix and fill with 1\n",
    "    topic_word_mat = np.full(shape=(num_topics, len(vocab)), fill_value=1)\n",
    "    # for each topic in the seed dict\n",
    "    topic2id = {topic: i for i, topic in enumerate(keywords)}\n",
    "    for topic, tokens in keywords.items(): \n",
    "        # for each seed token that is in vocab\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                topic_word_mat[topic2id[topic], vocab.token2id[token]] = pseudo_count\n",
    "    # denom = topic_word_mat.sum(axis=0)\n",
    "    # topic_word_mat = np.divide(topic_word_mat, pseudo_count)\n",
    "    return topic_word_mat\n",
    "\n",
    "topic_word_mat = create_topic_word_mat(keywords, vocab, len(keywords) + 1, len(corpus) // 100)\n",
    "\n",
    "topic_word_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e085663a-d6a1-4825-966f-da1f9a93bc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (self, words, numeric_metadata=[], metadata='', multi_metadata=[])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "inspect.signature(model.add_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07fb5c81-d3cb-499f-8250-a1b4b01eec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Documents:   0%|                                                                                                                          | 0/812951 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "function takes at most 3 arguments (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mGDMRModel(k\u001b[38;5;241m=\u001b[39mk, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, degrees\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(corpus, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdding Documents\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: function takes at most 3 arguments (4 given)"
     ]
    }
   ],
   "source": [
    "k = topic_word_mat.shape[0]\n",
    "model = tp.GDMRModel(k=k, seed=42, degrees=[1])\n",
    "\n",
    "for doc in tqdm.tqdm(corpus, desc='Adding Documents'):\n",
    "    model.add_doc(doc.tokens, numeric_metadata=[], metadata='', multi_metadata=[str(doc.author)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afc561-fd95-4510-a90d-50de7e06b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = model.eta\n",
    "for i, token in enumerate(model.vocabs):\n",
    "    model.set_word_prior(token, topic_word_mat[:, vocab.token2id[token]] + eta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a955d-06a7-4ba7-afee-584ca32b11a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_format = 'Traing Model - Iteration: {:>3.0f}, Log-likelihood: {:>3.4f}'\n",
    "iterations = 100\n",
    "chunksize = 10\n",
    "pbar = tqdm.tqdm(range(0, iterations, chunksize), desc=desc_format.format(0, 0))\n",
    "for i in pbar:\n",
    "    model.train(chunksize)\n",
    "    pbar.desc = desc_format.format(i + 1, model.ll_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c5982-f89a-434d-8673-60bc75bcd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(model.k):\n",
    "    print('Top 10 words of topic #{}'.format(k))\n",
    "    print(model.get_topic_words(k, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7b5a9-cc66-4447-89f6-973bd3726c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [model.make_doc(doc.tokens) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96235f5-ebd9-4f6d-87d7-1dc349b1445f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([4.5481458e-04, 3.0764390e-04, 2.8886209e-04, 6.0548410e-03,\n",
       "         1.6251026e-02, 9.7664279e-01], dtype=float32),\n",
       "  array([6.7029160e-04, 4.5339603e-04, 4.2571602e-04, 8.9234365e-03,\n",
       "         2.3950258e-02, 9.6557689e-01], dtype=float32),\n",
       "  array([1.9896778e-04, 1.3458502e-04, 1.2636854e-04, 2.6488120e-03,\n",
       "         9.9153662e-01, 5.3546205e-03], dtype=float32),\n",
       "  array([1.7443637e-04, 1.1799156e-04, 1.1078813e-04, 2.3222310e-03,\n",
       "         9.9258012e-01, 4.6944311e-03], dtype=float32),\n",
       "  array([2.3152816e-04, 1.5660937e-04, 1.4704831e-04, 9.8496121e-01,\n",
       "         8.2727568e-03, 6.2308852e-03], dtype=float32),\n",
       "  array([1.0791211e-04, 7.2993484e-05, 6.8537200e-05, 1.4366088e-03,\n",
       "         9.9540985e-01, 2.9041301e-03], dtype=float32),\n",
       "  array([1.7443637e-04, 1.1799156e-04, 1.1078813e-04, 2.3222310e-03,\n",
       "         9.9258012e-01, 4.6944311e-03], dtype=float32),\n",
       "  array([1.9896778e-04, 1.3458502e-04, 1.2636854e-04, 2.6488120e-03,\n",
       "         9.9153662e-01, 5.3546205e-03], dtype=float32),\n",
       "  array([1.7443637e-04, 1.1799156e-04, 1.1078813e-04, 9.8866957e-01,\n",
       "         6.2328037e-03, 4.6944311e-03], dtype=float32),\n",
       "  array([1.0791211e-04, 7.2993484e-05, 6.8537200e-05, 1.4366088e-03,\n",
       "         9.9540985e-01, 2.9041301e-03], dtype=float32)],\n",
       " array([-35.21585846, -21.38117957, -47.24327064, -52.84735155,\n",
       "        -55.41943669, -98.49545479, -82.50773668, -57.27616858,\n",
       "        -74.50104952, -97.97206306]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer(docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902acf60-1a9c-4ee7-94e6-532be29edeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef054c95-f824-466a-92e9-bb03f4c50869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
