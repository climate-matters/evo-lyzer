{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330a20b3-affe-4ab5-bf09-56acf8890d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c18420-4c62-4254-868b-dca5aae60a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eae6c09-d2f9-40e6-b95a-1660835455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.abspath('../../..') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9903ad93-7650-46ce-ac44-37b81e3c106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60eecb6b-17a5-4fd1-97c9-12a13f90427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    preprocess_string, \n",
    "    strip_tags,\n",
    "    strip_punctuation, \n",
    "    strip_multiple_whitespaces, \n",
    "    strip_numeric, \n",
    "    lower_to_unicode,\n",
    "    strip_short,\n",
    "    remove_stopwords,\n",
    ")\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import AuthorTopicModel as GensimAuthorTopicModel\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.test.utils import temporary_file\n",
    "from gensim.models.callbacks import CoherenceMetric, DiffMetric, PerplexityMetric, ConvergenceMetric\n",
    "\n",
    "from src.dataset import load_tweets, load_availability\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101cdb82-2345-45cf-9690-cf5138fd5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s', \n",
    "    level=logging.WARNING,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f8f68f-5c8a-42b8-8f02-ec5973c423ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(path=None):\n",
    "    if path is None:\n",
    "        path = '../../../data/external/keywords.v3.2.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    output = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        phrase, topic = row.raw_keyword, row.label\n",
    "        for token in phrase.split(' '):\n",
    "            if len(token) > 2:\n",
    "                output[topic].add(token)\n",
    "    return {k: list(v) for k, v in output.items()}\n",
    "\n",
    "keywords = load_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c557cd-fc1b-4d4d-b87a-f99a2bb35bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Documents: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 820202/820202 [02:04<00:00, 6605.61it/s]\n"
     ]
    }
   ],
   "source": [
    "class Document:\n",
    "    keyword_processor = None\n",
    "    url_pattern = re.compile('http[s]?://\\S+')\n",
    "    url_pattern_2 = re.compile('\\Bhttp[s]?\\S+')\n",
    "    hashtag_pattern = re.compile('\\B\\#[a-zA-Z0-9_]+')\n",
    "    mention_pattern = re.compile('\\B\\@[a-zA-Z0-9_]+')\n",
    "    \n",
    "    def __init__(self, text, author=None):\n",
    "        self._text = text\n",
    "        self._author = author\n",
    "        self._hashtags = Document.hashtag_pattern.findall(self._text)\n",
    "        self._tokens = preprocess_string(self._text, filters=[\n",
    "            lower_to_unicode,\n",
    "            lambda x: Document.url_pattern.sub(' ', x),\n",
    "            lambda x: Document.hashtag_pattern.sub(' ', x),\n",
    "            lambda x: Document.mention_pattern.sub(' ', x),\n",
    "            lambda x: Document.url_pattern_2.sub(' ', x),\n",
    "            strip_tags,\n",
    "            strip_punctuation,\n",
    "            strip_numeric,\n",
    "            lambda x: x + ' '.join(self._hashtags),\n",
    "            remove_stopwords,\n",
    "            strip_short,\n",
    "            strip_multiple_whitespaces, \n",
    "        ])\n",
    "        if Document.keyword_processor is None:\n",
    "            Document.keyword_processor = KeywordProcessor()\n",
    "            Document.keyword_processor.add_keywords_from_dict(keywords)\n",
    "        self.keywords = Document.keyword_processor.extract_keywords(self._text)\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self._text\n",
    "    \n",
    "    @property\n",
    "    def author(self):\n",
    "        return self._author\n",
    "    \n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return self._tokens\n",
    "    \n",
    "    @property\n",
    "    def has_keyword(self):\n",
    "        return len(self.keywords) > 0\n",
    "\n",
    "\n",
    "def load_documents(path=None):\n",
    "    if path is None:\n",
    "        path = '../../../data/interim/models/tweets_intra_subject_analysis.jsonl'\n",
    "    with open(path, 'r') as fp:\n",
    "        for line in tqdm.tqdm(fp.readlines(), desc='Loading Documents'):\n",
    "            record = json.loads(line)\n",
    "            text = record['tweet']['text']\n",
    "            author = record['subject_id']\n",
    "            d = Document(text=text, author=author)\n",
    "            yield d\n",
    "\n",
    "docs = []\n",
    "for doc in load_documents():\n",
    "    if len(doc.tokens) > 5:\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943b75cc-103d-4d4e-b572-04e8baca0353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Tokens: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 602863/602863 [00:00<00:00, 1062063.13it/s]\n",
      "Extracting Phrases: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 602863/602863 [00:27<00:00, 21708.23it/s]\n",
      "Extracting Author Docs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 602863/602863 [00:00<00:00, 874595.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 602863/602863 [00:16<00:00, 36913.87it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for i, doc in enumerate(tqdm.tqdm(docs, desc='Extracting Tokens')):\n",
    "    tokenized_docs.append(doc.tokens)\n",
    "\n",
    "# Add phrases to docs (only ones that appear 20 times or more).\n",
    "phrases_model_path = '../../../data/interim/models/phrase_model/v{}.pt'.format(MODEL_VERSION)\n",
    "try:\n",
    "    phrase_model = Phrases.load(phrases_model_path)\n",
    "except FileNotFoundError as ex: \n",
    "    phrase_model = Phrases(tokenized_docs, min_count=20, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "    phrase_model.freeze()\n",
    "    phrase_model.save(phrases_model_path)\n",
    "\n",
    "for idx in tqdm.tqdm(range(len(tokenized_docs)), desc='Extracting Phrases'):\n",
    "    for token in phrase_model[tokenized_docs[idx]]:\n",
    "        if '_' in token:\n",
    "            tokenized_docs[idx].append(token)\n",
    "    \n",
    "author2doc = defaultdict(list)\n",
    "for i, doc in enumerate(tqdm.tqdm(docs, desc='Extracting Author Docs')):\n",
    "    author2doc[doc.author].append(i)\n",
    "author2doc = dict(author2doc)\n",
    "        \n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "_ = dictionary[0]  # initialize dictionary.id2token\n",
    "\n",
    "corpus = []\n",
    "for tokenized_doc in tqdm.tqdm(tokenized_docs, desc='Extracting Bag of Words'):\n",
    "    corpus.append(dictionary.doc2bow(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbd90b64-5c20-4d51-b0f6-1b9b02656527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'causes': 52, 'problem': 124, 'solution': 60, 'description': 12, 'analysis': 40}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, 239707)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_eta(keywords, vocab, num_topics, pseudo_count=1e7, normalize=True, verbose=False):\n",
    "    # create a (ntopics, nterms) matrix and fill with 1\n",
    "    eta = np.full(shape=(num_topics, len(vocab)), fill_value=1)\n",
    "    # for each topic in the seed dict\n",
    "    topic2id = {topic: i for i, topic in enumerate(keywords)}\n",
    "    # for each topic in the seed dict\n",
    "    info = defaultdict(lambda: 0)\n",
    "    for topic, tokens in keywords.items(): \n",
    "        # for each seed token that is in vocab\n",
    "        for token in tokens:\n",
    "            if token in vocab.token2id:\n",
    "                info[topic] += 1\n",
    "                eta[topic2id[topic], vocab.token2id[token]] = pseudo_count\n",
    "    if verbose:\n",
    "        print(dict(info))\n",
    "    if normalize:\n",
    "        eta = np.divide(eta, eta.sum(axis=0))\n",
    "    return eta\n",
    "\n",
    "eta = create_eta(keywords, dictionary, len(keywords) + 1, len(corpus) // 100, normalize=False, verbose=True)\n",
    "\n",
    "eta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9daae715-6684-4385-a3f5-5a07e87d72c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mGensimAuthorTopicModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../../data/interim/models/topic_model/v\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_VERSION\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Set training parameters.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/gensim/models/ldamodel.py:1662\u001b[0m, in \u001b[0;36mLdaModel.load\u001b[0;34m(cls, fname, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1661\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmap\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1662\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLdaModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# check if `random_state` attribute has been set after main pickle load\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# if set -> the model to be loaded was saved using a >= 0.13.2 version of Gensim\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;66;03m# if not set -> the model to be loaded was saved using a < 0.13.2 version of Gensim,\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;66;03m# so set `random_state` as the default value\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/gensim/utils.py:485\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    483\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[0;32m--> 485\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/gensim/utils.py:1459\u001b[0m, in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/interim/models/topic_model/v5.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpasses\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunksize\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2000\u001b[39m,\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temporary_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialized\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m s_path:\n\u001b[0;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mGensimAuthorTopicModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauthor2doc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauthor2doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunksize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpasses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserialized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserialization_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../data/interim/models/topic_model/v\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(MODEL_VERSION))\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/gensim/models/atmodel.py:323\u001b[0m, in \u001b[0;36mAuthorTopicModel.__init__\u001b[0;34m(self, corpus, num_topics, id2word, author2doc, doc2author, chunksize, passes, iterations, decay, offset, alpha, eta, update_every, eval_every, gamma_threshold, serialized, serialization_path, minimum_probability, random_state)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (author2doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m doc2author \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    322\u001b[0m     use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor2doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc2author\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/gensim/models/atmodel.py:728\u001b[0m, in \u001b[0;36mAuthorTopicModel.update\u001b[0;34m(self, corpus, author2doc, doc2author, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If either doc2author or author2doc is missing, construct them from the other.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc2author \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 728\u001b[0m     doc2author \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_doc2author\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthor2doc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m author2doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     author2doc \u001b[38;5;241m=\u001b[39m construct_author2doc(doc2author)\n",
      "File \u001b[0;32m~/Documents/Projects/climate-matters/evo-lyzer/venv/lib/python3.9/site-packages/gensim/models/atmodel.py:124\u001b[0m, in \u001b[0;36mconstruct_doc2author\u001b[0;34m(corpus, author2doc)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(corpus):\n\u001b[1;32m    123\u001b[0m     author_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a, a_doc_ids \u001b[38;5;129;01min\u001b[39;00m author2doc\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m a_doc_ids:\n\u001b[1;32m    126\u001b[0m             author_ids\u001b[38;5;241m.\u001b[39mappend(a)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = GensimAuthorTopicModel.load('../../../data/interim/models/topic_model/v{}.pt'.format(MODEL_VERSION))\n",
    "except FileNotFoundError as ex:\n",
    "    # Set training parameters.\n",
    "    kwargs = {\n",
    "        'passes': 1,\n",
    "        'iterations': 1,\n",
    "        'chunksize': 2000,\n",
    "    }\n",
    "    with temporary_file('serialized') as s_path:\n",
    "        model = GensimAuthorTopicModel(\n",
    "            corpus,\n",
    "            author2doc=author2doc, \n",
    "            chunksize=kwargs.get('chunksize', 1000),\n",
    "            passes=kwargs.get('passes', 1),\n",
    "            iterations=kwargs.get('iterations', 50),\n",
    "            id2word=dictionary, \n",
    "            num_topics=eta.shape[0],\n",
    "            eta=eta,\n",
    "            serialized=True, \n",
    "            serialization_path=s_path,\n",
    "            eval_every=None,\n",
    "        )\n",
    "    model.save('../../../data/interim/models/topic_model/v{}.pt'.format(MODEL_VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c681ad-ac38-45fa-89e6-a72b8d01a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c7ad6-694f-4dac-8f03-3be71a6b43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the corpus.\n",
    "for topic_id in range(model.num_topics):\n",
    "    for term_id, p in model.get_topic_terms(topic_id):\n",
    "        print('{:>02} {:<30s}{:0.3f}'.format(topic_id, dictionary[term_id], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b84079-0994-4596-aab9-bddd02579d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tropic_proba = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(results))):\n",
    "    tropic_proba.append(list(zip(*model.get_new_author_topics(corpus[i:i+1])))[1])\n",
    "    \n",
    "tropic_proba = np.array(tropic_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79df44-33d5-453e-b985-53c7b82a110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tropic_proba.argmax(axis=1), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef52ab2-fbd1-4876-9ed0-179093b84524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import seaborn as sns\n",
    "from src.plotting import bokeh as bhp\n",
    "\n",
    "sns.set_theme()\n",
    "bhp.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e90626-0392-426d-99a1-4acedc93bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_data = [{'Topic': i, 'Probability': p} for i in range(tropic_proba.shape[1]) for p in tropic_proba[:, i]]\n",
    "\n",
    "fig_df = pd.DataFrame(fig_data)\n",
    "\n",
    "sns.displot(fig_df, x='Probability', row='Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5e350-f275-42ab-aa44-21c5ca3d0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "smallest_author = 0  # Ignore authors with documents less than this.\n",
    "authors = [model.author2id[a] for a in model.author2id.keys() if len(model.author2doc[a]) >= smallest_author]\n",
    "_ = tsne.fit_transform(model.state.gamma[authors, :])  # Result stored in tsne.embedding_\n",
    "\n",
    "x = tsne.embedding_[:, 0]\n",
    "y = tsne.embedding_[:, 1]\n",
    "author_names = [model.id2author[a] for a in authors]\n",
    "\n",
    "# Radius of each point corresponds to the number of documents attributed to that author.\n",
    "scale = 0.1\n",
    "author_sizes = [len(model.author2doc[a]) for a in author_names]\n",
    "radii = [size * scale for size in author_sizes]\n",
    "\n",
    "source = bhp.ColumnDataSource(\n",
    "    data=dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        author_names=author_names,\n",
    "        author_sizes=author_sizes,\n",
    "        radii=radii,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add author names and sizes to mouse-over info.\n",
    "hover = bhp.HoverTool(\n",
    "    tooltips=[\n",
    "        (\"author\", \"@author_names\"),\n",
    "        (\"size\", \"@author_sizes\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "p = bhp.figure(tools=[hover, 'crosshair,pan,wheel_zoom,box_zoom,reset,save,lasso_select'])\n",
    "p.scatter('x', 'y', radius='radii', source=source, fill_alpha=0.6, line_color=None)\n",
    "bhp.show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
