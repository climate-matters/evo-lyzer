{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5addb7c1-4740-43b9-a51c-7f96f84429d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efd64f0-f21c-482c-b4a7-37a7b9f72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcf7222-d029-47b3-80fb-35740c7da4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.abspath('../../..') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f505c7-6895-4797-b928-9d346d8057f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    preprocess_string, \n",
    "    strip_tags,\n",
    "    strip_punctuation, \n",
    "    strip_multiple_whitespaces, \n",
    "    strip_numeric, \n",
    "    lower_to_unicode,\n",
    "    strip_short,\n",
    "    remove_stopwords,\n",
    ")\n",
    "import tomotopy as tp\n",
    "\n",
    "from src.dataset import load_tweets, load_availability\n",
    "from src.utils.text import Document, Dictionary\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c091be4-082f-4776-bb87-101ee3ce8e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avx2\n"
     ]
    }
   ],
   "source": [
    "print(tp.isa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafe1d78-649f-41ac-a443-b0275cea3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(path=None):\n",
    "    if path is None:\n",
    "        path = '../../../data/external/keywords.v3.2.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    output = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        phrase, topic = row.raw_keyword, row.label\n",
    "        for token in phrase.split(' '):\n",
    "            if len(token) > 2:\n",
    "                output[topic].add(token)\n",
    "    return {k: list(v) for k, v in output.items()}\n",
    "\n",
    "keywords = load_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6727ff23-b431-40a2-b29b-3e7af582de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Documents: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 820202/820202 [00:49<00:00, 16429.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_documents(path=None):\n",
    "    if path is None:\n",
    "        path = '../../../data/interim/models/tweets_intra_subject_analysis.jsonl'\n",
    "    with open(path, 'r') as fp:\n",
    "        for line in tqdm.tqdm(fp.readlines(), desc='Loading Documents'):\n",
    "            record = json.loads(line)\n",
    "            text = record['tweet']['text']\n",
    "            author = record['subject_id']\n",
    "            # author = '{}-{}-{}'.format(record['subject_id'], record['event_id'], record['period'])\n",
    "            d = Document(text=text, author=author)\n",
    "            yield d\n",
    "\n",
    "url_pattern = re.compile('http[s]?://\\S+')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for doc in load_documents():\n",
    "    tokens = preprocess_string(doc.text, filters=[\n",
    "        lower_to_unicode,\n",
    "        lambda x: url_pattern.sub(' ', x),\n",
    "        strip_tags,\n",
    "        strip_punctuation,\n",
    "        strip_numeric, \n",
    "        remove_stopwords,\n",
    "        strip_short,\n",
    "        strip_multiple_whitespaces, \n",
    "    ])\n",
    "    doc.set_tokens(tokens)\n",
    "    if len(doc.tokens) > 0:\n",
    "        corpus.append(doc)\n",
    "        \n",
    "vocab = Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07e9b46a-e187-4bda-8e99-6714c519db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../../../data/interim/models/author2doc.json'):\n",
    "    with open('../../../data/interim/models/author2doc.json', 'r', encoding='utf-8') as fp:\n",
    "        author2doc = json.load(fp)\n",
    "else:\n",
    "    author2doc = defaultdict(list)\n",
    "    for i, doc in enumerate(tqdm.tqdm(results, desc='Extracting Tokens')):\n",
    "        author2doc[doc.author].append(i)\n",
    "    author2doc = dict(author2doc)\n",
    "    with open('../../../data/interim/models/author2doc.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(author2doc, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6cec81e9-c916-410b-9d57-23b82821592b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 318470)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_topic_word_mat(keywords, vocab, num_topics, pseudo_count=1e7):\n",
    "    # create a (ntopics, nterms) matrix and fill with 1\n",
    "    topic_word_mat = np.full(shape=(num_topics, len(vocab)), fill_value=1)\n",
    "    # for each topic in the seed dict\n",
    "    topic2id = {topic: i for i, topic in enumerate(keywords)}\n",
    "    for topic, tokens in keywords.items(): \n",
    "        # for each seed token that is in vocab\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                topic_word_mat[topic2id[topic], vocab.token2id[token]] = pseudo_count\n",
    "    # denom = topic_word_mat.sum(axis=0)\n",
    "    # topic_word_mat = np.divide(topic_word_mat, pseudo_count)\n",
    "    return topic_word_mat\n",
    "\n",
    "topic_word_mat = create_topic_word_mat(keywords, vocab, len(keywords) + 1, len(corpus) // 100)\n",
    "\n",
    "topic_word_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "07fb5c81-d3cb-499f-8250-a1b4b01eec09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Documents: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 812951/812951 [00:06<00:00, 131081.29it/s]\n"
     ]
    }
   ],
   "source": [
    "k = topic_word_mat.shape[0]\n",
    "model = tp.LDAModel(k=k, seed=42)\n",
    "\n",
    "for doc in tqdm.tqdm(corpus, desc='Adding Documents'):\n",
    "    model.add_doc(doc.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45afc561-fd95-4510-a90d-50de7e06b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = model.eta\n",
    "for i, token in enumerate(model.vocabs):\n",
    "    model.set_word_prior(token, topic_word_mat[:, vocab.token2id[token]] + eta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dd8a955d-06a7-4ba7-afee-584ca32b11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traing Model - Iteration:  91, Log-likelihood: -9.2750: 100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [02:51<00:00, 17.16s/it]\n"
     ]
    }
   ],
   "source": [
    "desc_format = 'Traing Model - Iteration: {:>3.0f}, Log-likelihood: {:>3.4f}'\n",
    "iterations = 100\n",
    "chunksize = 10\n",
    "pbar = tqdm.tqdm(range(0, iterations, chunksize), desc=desc_format.format(0, 0))\n",
    "for i in pbar:\n",
    "    model.train(chunksize)\n",
    "    pbar.desc = desc_format.format(i + 1, model.ll_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e96c5982-f89a-434d-8673-60bc75bcd7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words of topic #0\n",
      "[('gas', 0.03282236307859421), ('emissions', 0.027694014832377434), ('fossil', 0.024523762986063957), ('greenhouse', 0.02424403466284275), ('coal', 0.0201413556933403), ('carbon', 0.018649471923708916), ('fuel', 0.015013006515800953), ('fuels', 0.014733278192579746), ('plants', 0.01408057939261198), ('power', 0.01408057939261198)]\n",
      "Top 10 words of topic #1\n",
      "[('storm', 0.08679741621017456), ('proactive', 0.04691954702138901), ('text', 0.04659949988126755), ('warnings', 0.043591056019067764), ('potential', 0.04154275357723236), ('tracking', 0.041414737701416016), ('threat', 0.0412227064371109), ('sea', 0.019587524235248566), ('ice', 0.01581096649169922), ('area', 0.012866533361375332)]\n",
      "Top 10 words of topic #2\n",
      "[('energy', 0.03311057761311531), ('solar', 0.022377027198672295), ('carbon', 0.020436499267816544), ('wind', 0.01722249761223793), ('power', 0.016130950301885605), ('climate', 0.01603998802602291), ('global', 0.014311703853309155), ('change', 0.014099459163844585), ('clean', 0.01337176002562046), ('renewable', 0.01276534516364336)]\n",
      "Top 10 words of topic #3\n",
      "[('new', 0.0058397939428687096), ('people', 0.004690014757215977), ('amp', 0.004454811103641987), ('like', 0.003920449875295162), ('today', 0.0038117500953376293), ('covid', 0.0034364552702754736), ('story', 0.0034170113503932953), ('says', 0.0033397034276276827), ('time', 0.0031729056499898434), ('climate', 0.0030278947670012712)]\n",
      "Top 10 words of topic #4\n",
      "[('rain', 0.013404754921793938), ('weather', 0.013038928620517254), ('morning', 0.011817777529358864), ('today', 0.01011978555470705), ('storm', 0.009391054511070251), ('snow', 0.008351030759513378), ('severe', 0.00832830835133791), ('day', 0.007856013253331184), ('forecast', 0.0077615538612008095), ('storms', 0.007684947922825813)]\n",
      "Top 10 words of topic #5\n",
      "[('thanks', 0.006132778245955706), ('thank', 0.005463317036628723), ('las', 0.004646900575608015), ('morning', 0.00462240818887949), ('que', 0.004408655688166618), ('day', 0.004152597859501839), ('love', 0.003973728511482477), ('los', 0.0038393905851989985), ('happy', 0.003807476256042719), ('good', 0.0036902092397212982)]\n"
     ]
    }
   ],
   "source": [
    "for k in range(model.k):\n",
    "    print('Top 10 words of topic #{}'.format(k))\n",
    "    print(model.get_topic_words(k, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cdca3ca8-b6e6-4c11-81c4-35ecd290e8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Basic Info>\n",
      "| LDAModel (current version: 0.12.2)\n",
      "| 812951 docs, 8736916 words\n",
      "| Total Vocabs: 318470, Used Vocabs: 318470\n",
      "| Entropy of words: 9.15080\n",
      "| Entropy of term-weighted words: 9.15080\n",
      "| Removed Vocabs: <NA>\n",
      "|\n",
      "<Training Info>\n",
      "| Iterations: 100, Burn-in steps: 0\n",
      "| Optimization Interval: 10\n",
      "| Log-likelihood per word: -9.27501\n",
      "|\n",
      "<Initial Parameters>\n",
      "| tw: TermWeight.ONE\n",
      "| min_cf: 0 (minimum collection frequency of words)\n",
      "| min_df: 0 (minimum document frequency of words)\n",
      "| rm_top: 0 (the number of top words to be removed)\n",
      "| k: 6 (the number of topics between 1 ~ 32767)\n",
      "| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n",
      "| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n",
      "| seed: 42 (random seed)\n",
      "| trained in version 0.12.2\n",
      "|\n",
      "<Parameters>\n",
      "| alpha (Dirichlet prior on the per-document topic distributions)\n",
      "|  [0.00024405 0.00043354 0.00437627 0.08182476 0.05522852 0.03460424]\n",
      "| eta (Dirichlet prior on the per-topic word distribution)\n",
      "|  0.01\n",
      "|\n",
      "<Topics>\n",
      "| #0 (7540) : gas emissions fossil greenhouse coal\n",
      "| #1 (12438) : storm proactive text warnings potential\n",
      "| #2 (29796) : energy solar carbon wind power\n",
      "| #3 (4265461) : new people amp like today\n",
      "| #4 (3077514) : rain weather morning today storm\n",
      "| #5 (1344167) : thanks thank las morning que\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7b5a9-cc66-4447-89f6-973bd3726c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
