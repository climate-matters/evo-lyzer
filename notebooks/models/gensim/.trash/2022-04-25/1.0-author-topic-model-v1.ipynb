{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330a20b3-affe-4ab5-bf09-56acf8890d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c18420-4c62-4254-868b-dca5aae60a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eae6c09-d2f9-40e6-b95a-1660835455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.abspath('../..') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "60eecb6b-17a5-4fd1-97c9-12a13f90427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    preprocess_string, \n",
    "    strip_tags,\n",
    "    strip_punctuation, \n",
    "    strip_multiple_whitespaces, \n",
    "    strip_numeric, \n",
    "    lower_to_unicode,\n",
    "    strip_short,\n",
    "    remove_stopwords,\n",
    ")\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# from gensim.models.ldamodel import LdaModel as _LdaModel\n",
    "from gensim.models import AuthorTopicModel as GensimAuthorTopicModel\n",
    "from gensim.test.utils import temporary_file\n",
    "\n",
    "from src.dataset import load_tweets, load_availability\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8f68f-5c8a-42b8-8f02-ec5973c423ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(path=None):\n",
    "    if path is None:\n",
    "        path = '../../data/external/keywords.v3.2.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    output = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        phrase, topic = row.raw_keyword, row.label\n",
    "        for token in phrase.split(' '):\n",
    "            if len(token) > 2:\n",
    "                output[topic].add(token)\n",
    "    return {k: list(v) for k, v in output.items()}\n",
    "\n",
    "keywords = load_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6275463f-cedb-44b0-bd47-cb0009f62e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Documents: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 820202/820202 [00:51<00:00, 15845.21it/s]\n"
     ]
    }
   ],
   "source": [
    "class Document:\n",
    "    def __init__(self, text, author=None):\n",
    "        self.text = text\n",
    "        self.author = author\n",
    "        self.tokens = None\n",
    "        \n",
    "    def set_tokens(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        return self\n",
    "\n",
    "def load_documents(path=None):\n",
    "    if path is None:\n",
    "        path = '../../data/interim/models/tweets_intra_subject_analysis.jsonl'\n",
    "    with open(path, 'r') as fp:\n",
    "        for line in tqdm.tqdm(fp.readlines(), desc='Loading Documents'):\n",
    "            record = json.loads(line)\n",
    "            text = record['tweet']['text']\n",
    "            author = record['subject_id']\n",
    "            # author = '{}-{}-{}'.format(record['subject_id'], record['event_id'], record['period'])\n",
    "            d = Document(text=text, author=author)\n",
    "            yield d\n",
    "\n",
    "url_pattern = re.compile('http[s]?://\\S+')\n",
    "\n",
    "results = []\n",
    "\n",
    "for doc in load_documents():\n",
    "    tokens = preprocess_string(doc.text, filters=[\n",
    "        lower_to_unicode,\n",
    "        lambda x: url_pattern.sub(' ', x),\n",
    "        strip_tags,\n",
    "        strip_punctuation,\n",
    "        strip_numeric, \n",
    "        remove_stopwords,\n",
    "        strip_short,\n",
    "        strip_multiple_whitespaces, \n",
    "    ])\n",
    "    doc.set_tokens(tokens)\n",
    "    if len(doc.tokens) > 0:\n",
    "        results.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b75cc-103d-4d4e-b572-04e8baca0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../../data/interim/models/author2doc.json'):\n",
    "    with open('../../data/interim/models/author2doc.json', 'r', encoding='utf-8') as fp:\n",
    "        author2doc = json.load(fp)\n",
    "else:\n",
    "    author2doc = defaultdict(list)\n",
    "    tokenized_docs = []\n",
    "    for i, doc in enumerate(tqdm.tqdm(results, desc='Extracting Tokens')):\n",
    "        author2doc[doc.author].append(i)\n",
    "        tokenized_docs.append(doc.tokens)\n",
    "    author2doc = dict(author2doc)\n",
    "    with open('../../data/interim/models/author2doc.json', 'w', encoding='utf-8') as fp:\n",
    "        json.dump(author2doc, fp)\n",
    "        \n",
    "if os.path.exists('../../data/interim/models/dictionary.pk'):\n",
    "    dictionary = Dictionary.load('../../data/interim/models/dictionary.pk')\n",
    "else:\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "    dictionary.save('../../data/interim/models/dictionary.pk')\n",
    "\n",
    "corpus = [dictionary.doc2bow(tokenized_doc) for tokenized_doc in tqdm.tqdm(tokenized_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbd90b64-5c20-4d51-b0f6-1b9b02656527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 318470)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_eta(keywords, vocab, num_topics, pseudo_count=1e7):\n",
    "    # create a (ntopics, nterms) matrix and fill with 1\n",
    "    eta = np.full(shape=(num_topics, len(vocab)), fill_value=1)\n",
    "    # for each topic in the seed dict\n",
    "    for topic, tokens in keywords.items(): \n",
    "        # for each seed token that is in vocab\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                eta[topic, vocab.token2id[token]] = pseudo_count\n",
    "    return np.divide(eta, eta.sum(axis=0))\n",
    "\n",
    "eta = create_eta(keywords, dictionary, len(keywords) + 1, len(corpus) // 100)\n",
    "\n",
    "eta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9daae715-6684-4385-a3f5-5a07e87d72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with temporary_file('serialized') as s_path:\n",
    "    model = GensimAuthorTopicModel(\n",
    "        corpus,\n",
    "        author2doc=author2doc, \n",
    "        id2word=dictionary, \n",
    "        num_topics=eta.shape[0],\n",
    "        eta=eta,\n",
    "        serialized=True, \n",
    "        serialization_path=s_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69c681ad-ac38-45fa-89e6-a72b8d01a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "489c7ad6-694f-4dac-8f03-3be71a6b43fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 trump                         0.006\n",
      "00 wlrn                          0.005\n",
      "00 nahj                          0.003\n",
      "00 journalists                   0.003\n",
      "00 highcountrynews               0.003\n",
      "00 city                          0.003\n",
      "00 journalism                    0.003\n",
      "00 stories                       0.003\n",
      "00 craigtimes                    0.003\n",
      "00 proactive                     0.002\n",
      "01 broward                       0.009\n",
      "01 federal                       0.008\n",
      "01 hospital                      0.005\n",
      "01 fort                          0.005\n",
      "01 staff                         0.005\n",
      "01 conference                    0.005\n",
      "01 communities                   0.005\n",
      "01 tested                        0.004\n",
      "01 editor                        0.004\n",
      "01 chief                         0.004\n",
      "02 que                           0.008\n",
      "02 puerto                        0.006\n",
      "02 los                           0.005\n",
      "02 politeicecream                0.005\n",
      "02 rico                          0.004\n",
      "02 jgbm                          0.003\n",
      "02 las                           0.003\n",
      "02 rebeccaaguilar                0.003\n",
      "02 por                           0.003\n",
      "02 coacheswaiting                0.003\n",
      "03 lauderdale                    0.008\n",
      "03 wralweather                   0.004\n",
      "03 workshop                      0.003\n",
      "03 manatee                       0.003\n",
      "03 mynewsweather                 0.003\n",
      "03 hillsborough                  0.003\n",
      "03 pinellas                      0.003\n",
      "03 wral                          0.003\n",
      "03 gawx                          0.003\n",
      "03 moth                          0.002\n",
      "04 new                           0.010\n",
      "04 covid                         0.009\n",
      "04 florida                       0.009\n",
      "04 people                        0.008\n",
      "04 amp                           0.008\n",
      "04 like                          0.007\n",
      "04 today                         0.007\n",
      "04 storm                         0.007\n",
      "04 time                          0.006\n",
      "04 story                         0.006\n",
      "05 thank                         0.010\n",
      "05 graphics                      0.007\n",
      "05 students                      0.006\n",
      "05 law                           0.004\n",
      "05 sorry                         0.003\n",
      "05 city                          0.003\n",
      "05 fun                           0.003\n",
      "05 friends                       0.003\n",
      "05 writing                       0.003\n",
      "05 you’re                        0.003\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the corpus.\n",
    "for topic_id in range(model.num_topics):\n",
    "    for term_id, p in model.get_topic_terms(topic_id):\n",
    "        print('{:>02} {:<30s}{:0.3f}'.format(topic_id, dictionary[term_id], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fab7679a-ec08-4d3e-a38f-0e3bf7716827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../data/interim/models/models/v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cb5cb-2a9d-43e8-b853-b29d317e7557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
