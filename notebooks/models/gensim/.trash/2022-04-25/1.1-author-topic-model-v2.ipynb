{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330a20b3-affe-4ab5-bf09-56acf8890d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c18420-4c62-4254-868b-dca5aae60a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eae6c09-d2f9-40e6-b95a-1660835455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.abspath('../..') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60eecb6b-17a5-4fd1-97c9-12a13f90427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    preprocess_string, \n",
    "    strip_tags,\n",
    "    strip_punctuation, \n",
    "    strip_multiple_whitespaces, \n",
    "    strip_numeric, \n",
    "    lower_to_uniDictionary,\n",
    "    strip_short,\n",
    "    remove_stopwords,\n",
    ")\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import AuthorTopicModel as GensimAuthorTopicModel\n",
    "from gensim.test.utils import temporary_file\n",
    "\n",
    "from src.dataset import load_tweets, load_availability\n",
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f8f68f-5c8a-42b8-8f02-ec5973c423ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(path=None):\n",
    "    if path is None:\n",
    "        path = '../../data/external/keywords.v3.2.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    output = defaultdict(set)\n",
    "    for row in df.itertuples():\n",
    "        phrase, topic = row.raw_keyword, row.label\n",
    "        for token in phrase.split(' '):\n",
    "            if len(token) > 2:\n",
    "                output[topic].add(token)\n",
    "    return {k: list(v) for k, v in output.items()}\n",
    "\n",
    "keywords = load_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa56af-521d-4d96-833d-c4762084423f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis', 'causes', 'description', 'problem', 'solution'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_processor = KeywordProcessor()\n",
    "\n",
    "keyword_processor.add_keywords_from_dict(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6275463f-cedb-44b0-bd47-cb0009f62e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Documents: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 820202/820202 [01:10<00:00, 11600.77it/s]\n"
     ]
    }
   ],
   "source": [
    "class Document:\n",
    "    def __init__(self, text, author=None):\n",
    "        self.text = text\n",
    "        self.author = author\n",
    "        self.tokens = None\n",
    "        \n",
    "    def set_tokens(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        return self\n",
    "\n",
    "def load_documents(path=None):\n",
    "    if path is None:\n",
    "        path = '../../data/interim/models/tweets_intra_subject_analysis.jsonl'\n",
    "    with open(path, 'r') as fp:\n",
    "        for line in tqdm.tqdm(fp.readlines(), desc='Loading Documents'):\n",
    "            record = json.loads(line)\n",
    "            text = record['tweet']['text']\n",
    "            author = record['subject_id']\n",
    "            d = Document(text=text, author=author)\n",
    "            yield d\n",
    "\n",
    "url_pattern = re.compile('http[s]?://\\S+')\n",
    "\n",
    "results = []\n",
    "\n",
    "for doc in load_documents():\n",
    "    keywords_found = keyword_processor.extract_keywords(doc.text)\n",
    "    if len(keywords_found) == 0:\n",
    "        continue\n",
    "    tokens = preprocess_string(doc.text, filters=[\n",
    "        lower_to_unicode,\n",
    "        lambda x: url_pattern.sub(' ', x),\n",
    "        strip_tags,\n",
    "        strip_punctuation,\n",
    "        strip_numeric, \n",
    "        remove_stopwords,\n",
    "        strip_short,\n",
    "        strip_multiple_whitespaces, \n",
    "    ])\n",
    "    doc.set_tokens(tokens)\n",
    "    if len(doc.tokens) > 0:\n",
    "        results.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "943b75cc-103d-4d4e-b572-04e8baca0353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Tokens: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 478927/478927 [00:00<00:00, 1704669.45it/s]\n",
      "Extracting Author Docs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 478927/478927 [00:00<00:00, 1728318.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 478927/478927 [00:08<00:00, 58655.08it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for i, doc in enumerate(tqdm.tqdm(results, desc='Extracting Tokens')):\n",
    "    tokenized_docs.append(doc.tokens)\n",
    "\n",
    "author2doc = defaultdict(list)\n",
    "for i, doc in enumerate(tqdm.tqdm(results, desc='Extracting Author Docs')):\n",
    "    author2doc[doc.author].append(i)\n",
    "author2doc = dict(author2doc)\n",
    "        \n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "corpus = [dictionary.doc2bow(tokenized_doc) for tokenized_doc in tqdm.tqdm(tokenized_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbd90b64-5c20-4d51-b0f6-1b9b02656527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 211574)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_eta(keywords, vocab, num_topics, pseudo_count=1e7):\n",
    "    # create a (ntopics, nterms) matrix and fill with 1\n",
    "    eta = np.full(shape=(num_topics, len(vocab)), fill_value=1)\n",
    "    # for each topic in the seed dict\n",
    "    for topic, tokens in keywords.items(): \n",
    "        # for each seed token that is in vocab\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                eta[topic, vocab.token2id[token]] = pseudo_count\n",
    "    return np.divide(eta, eta.sum(axis=0))\n",
    "\n",
    "eta = create_eta(keywords, dictionary, len(keywords) + 1, len(corpus) // 100)\n",
    "\n",
    "eta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9daae715-6684-4385-a3f5-5a07e87d72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with temporary_file('serialized') as s_path:\n",
    "    model = GensimAuthorTopicModel(\n",
    "        corpus,\n",
    "        author2doc=author2doc, \n",
    "        id2word=dictionary, \n",
    "        num_topics=eta.shape[0],\n",
    "        eta=eta,\n",
    "        serialized=True, \n",
    "        serialization_path=s_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69c681ad-ac38-45fa-89e6-a72b8d01a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_vecs = [model.get_author_topics(author) for author in model.id2author.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "489c7ad6-694f-4dac-8f03-3be71a6b43fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 que                           0.002\n",
      "00 floridamuseum                 0.001\n",
      "00 russcontreras                 0.000\n",
      "00 maryjowebster                 0.000\n",
      "00 rebeccaaguilar                0.000\n",
      "00 seashells                     0.000\n",
      "00 ensiamedia                    0.000\n",
      "00 jretis                        0.000\n",
      "00 por                           0.000\n",
      "00 arelisrhdz                    0.000\n",
      "01 civilbeat                     0.002\n",
      "01 hinews                        0.001\n",
      "01 ndn                           0.001\n",
      "01 nmpol                         0.000\n",
      "01 wfdd                          0.000\n",
      "01 thenewspress                  0.000\n",
      "01 nathaneagle                   0.000\n",
      "01 nickgrube                     0.000\n",
      "01 chadgillisnp                  0.000\n",
      "01 patjriley                     0.000\n",
      "02 storm                         0.025\n",
      "02 tropical                      0.015\n",
      "02 potential                     0.008\n",
      "02 nhc                           0.007\n",
      "02 track                         0.007\n",
      "02 hurricane                     0.007\n",
      "02 morning                       0.007\n",
      "02 threat                        0.006\n",
      "02 today                         0.006\n",
      "02 newest                        0.006\n",
      "03 wusf                          0.001\n",
      "03 schwartznewsny                0.001\n",
      "03 paullarocco                   0.001\n",
      "03 verachinese                   0.000\n",
      "03 davidolson                    0.000\n",
      "03 bethpage                      0.000\n",
      "03 grumman                       0.000\n",
      "03 scotteidler                   0.000\n",
      "03 nassau                        0.000\n",
      "03 scottyeidz                    0.000\n",
      "04 new                           0.007\n",
      "04 covid                         0.006\n",
      "04 people                        0.005\n",
      "04 amp                           0.005\n",
      "04 florida                       0.004\n",
      "04 climate                       0.004\n",
      "04 story                         0.004\n",
      "04 says                          0.003\n",
      "04 state                         0.003\n",
      "04 like                          0.003\n",
      "05 i’m                           0.005\n",
      "05 love                          0.004\n",
      "05 know                          0.004\n",
      "05 like                          0.003\n",
      "05 thank                         0.003\n",
      "05 got                           0.003\n",
      "05 it’s                          0.003\n",
      "05 people                        0.002\n",
      "05 atlanta                       0.002\n",
      "05 work                          0.002\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the corpus.\n",
    "for topic_id in range(model.num_topics):\n",
    "    for term_id, p in model.get_topic_terms(topic_id):\n",
    "        print('{:>02} {:<30s}{:0.3f}'.format(topic_id, dictionary[term_id], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab7679a-ec08-4d3e-a38f-0e3bf7716827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../data/interim/models/models/v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a75016-5790-4bc3-a8ab-30999536a8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
